{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface cache is in ./cache\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = './cache'\n",
    "print(\"huggingface cache is in {}\".format(os.getenv('HF_HOME')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/kqa3/anaconda3/envs/llama/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = 'NousResearch/Llama-2-7b-chat-hf'\n",
    "new_model = \"./security_miles_model/baseline_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.42it/s]\n",
      "/scratch/kqa3/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/scratch/kqa3/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/scratch/kqa3/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/scratch/kqa3/anaconda3/envs/llama/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:415: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "load_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    "    # some paramerters from https://discuss.huggingface.co/t/help-with-llama-2-finetuning-setup/50035/3\n",
    "    temperature=1.0,\n",
    "    # do_sample=True,gggg\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "model = PeftModel.from_pretrained(load_model, new_model)\n",
    "# model = PeftModel.from_pretrained(load_model)\n",
    "# model = model.merge_and_unload()\n",
    "\n",
    "\n",
    "# from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "# # Load the Llama model\n",
    "# model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "#     low_cpu_mem_usage=True,\n",
    "#     return_dict=True,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map={\"\": 0},\n",
    "#     # some paramerters from https://discuss.huggingface.co/t/help-with-llama-2-finetuning-setup/50035/3\n",
    "#     temperature=1.0,\n",
    "#     # do_sample=True,gggg\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] distinguish between software attacks and hardware attacks? [/INST] Software attacks are attacks that target software, such as malware, viruses, and other types of malicious code.\n",
      "\n",
      "Hardware attacks are attacks that target hardware, such as physical tampering with a device or compromising the hardware components of a system.\n",
      "\n",
      "Software attacks can be launched through various means, including:\n",
      "\n",
      "* Malware: Malware is a general term for any type of malicious software that can harm a computer system. Malware can be installed on a computer through various means, including email attachments, infected software downloads, and infected websites.\n",
      "* Viruses: A virus is a type of malware that replicates itself and can spread to other computers. Viruses can be spread through email attachments, infected software downloads, and infected websites.\n",
      "* Worms: A worm is a type of malware that\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"distinguish between software attacks and hardware attacks?\"\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] what's CIA Triad?  [/INST] The CIA Triad is a framework used to describe the three main goals of an organization's information security program: Confidentiality, Integrity, and Availability.\n",
      "\n",
      "Confidentiality refers to the protection of sensitive information from unauthorized access.\n",
      "\n",
      "Integrity refers to the protection of information from unauthorized modification or destruction.\n",
      "\n",
      "Availability refers to the assurance that information is accessible and usable when needed.\n",
      "\n",
      "The CIA Triad is often used as a starting point for organizations to assess their information security posture and identify areas for improvement.\n",
      "\n",
      "It is important to note that the CIA Triad is not a static framework, and organizations should regularly assess and update their information security program to ensure that it is aligned with their business objectives and the evolving threat landscape.\n",
      "\n",
      "In addition to the CIA Triad\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"what's CIA Triad? \"\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Who is Leonardo Da Vinci? [/INST] the account of a user who had voted on your question has been deleted. sometimes users do this themselves, rarely we have to do it for them. when they delete their account, everything they touched goes away with them. see also what does &quot;user was removed&quot; mean and why did my reputation change because of it?.\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Who is Leonardo Da Vinci?\"\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] What is result of 1+1? [/INST] this is not a security issue. this is a programming issue. the code is not doing what you want, so you are asking where the code is going wrong. from the looks of it, the code is following the logic that you have provided. you want the code to do something different. this is not a security issue, this is a programming issue. you are getting unexpected results because the code is following the logic that you have provided. this is not a security issue, this is a programming issue. your question is not about security, but about how your code is not being followed. your question is not about security, but about how your code is not being followed. your question is not about security, but about how your code is not being followed. your question is not about security, but about how your code is not being followed. your question is not about security, but about how your code is\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is result of 1+1?\"\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] What's shellcode? and what's relative addressing technique when you want to implement spwning a shell? [/INST] \"nuclear option\" is a metaphor for wiping all local storage, overwriting all firmware, and physically disabling the device. this is useful for preventing attackers from using the device for malicious activities after a compromise, but it is also a lot of damage for no apparent reason. why would a security professional want to destroy devices they are servicing? if an attacker is able to gain remote access to a device, wiping its local storage and disabling it is a lot easier than physically disabling the device. if an attacker can physically access the device, disabling it permanently is a lot easier than trying to recover it after a compromise. if you are in a position where you are servicing devices and destroying them, you should be\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What's shellcode? and what's relative addressing technique when you want to implement spwning a shell?\"\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] what is a hacker? [/INST] the hacker is someone who appreciates the aesthetics of broken things. often, this is applied to computer systems, where a hacker might enjoy exploring the details of an exploit, or a backdoor, or an elevated privilege level. but in the information security field, we are also exposed to broken things on a daily basis, whether it is a misconfigured server, or a policy that needs tweaking, or a desperate user who is frustrated with the state of security in the world. the hacker, then, is someone who is comfortable with the complex relationships between people, systems, and information, and who enjoys exploring the aesthetics of broken things, and finding the hidden beauty in the complex relationships between technology and society. so what does this mean in practical terms? it means that when we are discussing a particularly nasty piece of malware\n"
     ]
    }
   ],
   "source": [
    "prompt = \"what is a hacker?\"\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Can you use GPG to decrypt anbd encrypt with a different key in one step? [/INST] the encryption and decryption are performed separately. the encryption/decryption algorithm is based on aes and aes-cmc. the encryption/decryption key is derived from the user's password. the user can set a very long (but reasonably short) aes key, like 30-40 digits, unchanged during runtime. the user can set an identical, very long, fixed aes-cmc key. the user can set an identical, very long fixed aes key for encryption and decryption. the user can set a very long, fixed aes key for the aes-cmc. the user can set a very long, fixed aes key for the encryption and decryption. the user can set a very long, fixed aes key for the aes-cmc. the user can set\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Can you use GPG to decrypt anbd encrypt with a different key in one step?\"\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] How to securely store secrets in Docker container? [/INST] 140 members of security professionals from around the world gathered in dubai for the esfu security forum. the event was held under the patronage of his excellency sheikh hamdan bin mubarak al-mu’alla, the crown prince of dubai, and in partnership with etisalat. the conference addressed issues related to physical security, information security, risk management and other aspects of security. some of the highlights: keynote speaker: mohammed al-muhairi, uae national and ceo of etisalat speaking session: industry experts including mohammed al-muhairi, rory mccoy, steve hurn, and others breakout sessions: physical security, information security, risk management and other topics panel discussions: cyberterrorism; the future of physical security; the future\n"
     ]
    }
   ],
   "source": [
    "prompt = \"How to securely store secrets in Docker container?\"\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
