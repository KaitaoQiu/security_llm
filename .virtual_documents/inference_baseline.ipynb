import torch
print(torch.cuda.is_available())


import os
os.environ['HF_HOME'] = './cache'
print("huggingface cache is in {}".format(os.getenv('HF_HOME')))


import os
import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    HfArgumentParser,
    TrainingArguments,
    pipeline,
    logging,
)
from peft import LoraConfig, PeftModel
from trl import SFTTrainer


base_model = 'NousResearch/Llama-2-7b-chat-hf'
new_model = "./security_miles_model/baseline_model"


# Reload model in FP16 and merge it with LoRA weights
load_model = AutoModelForCausalLM.from_pretrained(
    base_model,
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.float16,
    device_map={"": 0},
    # some paramerters from https://discuss.huggingface.co/t/help-with-llama-2-finetuning-setup/50035/3
    temperature=1.0,
    # do_sample=True,gggg

)


model = PeftModel.from_pretrained(load_model, new_model)
# model = PeftModel.from_pretrained(load_model)
# model = model.merge_and_unload()


# from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer

# # Load the Llama model
# model_name = "NousResearch/Llama-2-7b-chat-hf"
# model = AutoModelForCausalLM.from_pretrained(model_name, 
#     low_cpu_mem_usage=True,
#     return_dict=True,
#     torch_dtype=torch.float16,
#     device_map={"": 0},
#     # some paramerters from https://discuss.huggingface.co/t/help-with-llama-2-finetuning-setup/50035/3
#     temperature=1.0,
#     # do_sample=True,gggg
#     )



# Reload tokenizer to save it
tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
tokenizer.add_special_tokens({'pad_token': '[PAD]'})
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = "right"


# Ignore warnings
logging.set_verbosity(logging.CRITICAL)
pipe = pipeline(task="text-generation", model=model, tokenizer=tokenizer, max_length=200)


# Run text generation pipeline with our next model
prompt = "distinguish between software attacks and hardware attacks?"
result = pipe(f"<s>[INST] {prompt} [/INST]")
print(result[0]['generated_text'])


# Run text generation pipeline with our next model
prompt = "what's CIA Triad? "
result = pipe(f"<s>[INST] {prompt} [/INST]")
print(result[0]['generated_text'])














# Run text generation pipeline with our next model
prompt = "Who is Leonardo Da Vinci?"
result = pipe(f"<s>[INST] {prompt} [/INST]")
print(result[0]['generated_text'])


prompt = "What is result of 1+1?"
result = pipe(f"<s>[INST] {prompt} [/INST]")
print(result[0]['generated_text'])


prompt = "What's shellcode? and what's relative addressing technique when you want to implement spwning a shell?"
result = pipe(f"<s>[INST] {prompt} [/INST]")
print(result[0]['generated_text'])


prompt = "what is a hacker?"
result = pipe(f"<s>[INST] {prompt} [/INST]")
print(result[0]['generated_text'])


prompt = "Can you use GPG to decrypt anbd encrypt with a different key in one step?"
result = pipe(f"<s>[INST] {prompt} [/INST]")
print(result[0]['generated_text'])


prompt = "How to securely store secrets in Docker container?"
result = pipe(f"<s>[INST] {prompt} [/INST]")
print(result[0]['generated_text'])



